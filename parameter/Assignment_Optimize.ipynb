{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction\n",
    "\n",
    "---\n",
    "Up to this point, we’ve covered the foundational concepts of machine learning. You may have already experimented with various hyperparameters to optimize model performance. In this notebook, you’ll be introduced to several advanced techniques designed to further enhance your models.\n",
    "\n",
    "\n",
    "This notebook consists the following parts:\n",
    "\n",
    "- [A: Data retrieval ](#01)\n",
    "- [B: Feature engineering](#02)\n",
    "- [C: Model evaluation](#03)\n",
    "- [D: Ensemble learning](#04)\n",
    "- [E: Pipelines](#05)\n",
    "- [F: Bring it all together](#06)\n",
    "- [G: Bonus: ML OPS](#07)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this two weeks you will be able to:\n",
    "- Understand the fundamental concepts of ensemble learning.\n",
    "- Use evaluation techniques to assess models performance.\n",
    "- Enhance model performance by feature engineering. \n",
    "- Build pipelines for model development and preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "### Instructions\n",
    "- Ensure you fully understand the requirements and objectives of the assignment.\n",
    "- Review the notebooks refered in the tasks\n",
    "- If you need additional context or clarification, please check the provided videos or background literature.\n",
    "- Work through each part of the assignment methodically, ensuring all tasks are completed.\n",
    "- Update your repository with your new created work\n",
    "\n",
    "### Additional Notes:\n",
    "- Do not add datafiles to your repository. Repositories with datafiles will not be accepted\n",
    "- Class solutions should be delivered in python files. Not in notebooks\n",
    "- When AI tools are used, you must provide proper references and explanations for how they were utilized. Failure to do so will be considered as academic fraud\n",
    "- The bonus assignment are not mandatory\n",
    "- Use PEP8 \n",
    "\n",
    "Good luck!\n",
    "\n",
    "F.Feenstra\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='01'></a>\n",
    "## Part A. Data retrieval\n",
    "\n",
    "The dataset you can use for this notebook is the lung dataset from Maastricht University. It comprises 89 non-small cell lung cancer (NSCLC) patients records who underwent surgical treatment. The study where the data is from explored the relationship between radiomic imaging features and gene expression profiles. The samples were collected through biopsies at MAASTRO Clinic in The Netherlands, and the dataset is publicly available.\n",
    "\n",
    "The authors of the related paper discovered that a prognostic radiomic signature, which captures intra-tumor heterogeneity, is closely associated with underlying gene expression patterns. Developing a machine learning model to predict histology from the Clinical and Genetic Lung data can improve diagnostic accuracy and treatment personalization. In this notebook we will develop such a prediction model. \n",
    "\n",
    "**Availabel Datasets**:\n",
    "- Lung metadata dataset [1]\n",
    "- Gene expression dataset [2]\n",
    "\n",
    "**Important**\n",
    "<span style=\"background-color: lightgreen;color: black\">It is also allowed to use your own dataset from your own project if this data is highly dimensional and contains genetic information.</span>\n",
    "\n",
    "[1] [NSCLC-Radiomics-Genomics](https://wiki.cancerimagingarchive.net/display/Public/NSCLC-Radiomics-Genomics#16056856db10d39adf704eefa53e41edcf5ef41c)\n",
    "\n",
    "[2] [Gene Expression Data - GSE58661](https://ftp.ncbi.nlm.nih.gov/geo/series/GSE58nnn/GSE58661/matrix/)\n",
    "\n",
    "[3] Aerts HJWL, Rios Velazquez E, Leijenaar RTH, Parmar C, Grossmann P, Carvalho S, Bussink J, Monshouwer R, Haibe-Kains B, Rietveld D, Hoebers F, Rietbergen MM, Leemans CR, Dekker A, Quackenbush J, Gillies RJ, & Lambin P. (2015). Data From NSCLC-Radiomics-Genomics. The Cancer Imaging Archive. https://doi.org/10.7937/K9/TCIA.2015.L4FRET6Z\n",
    "\n",
    "\n",
    "### <span style=\"background-color: lightyellow;\">Data retrieval task</span>\n",
    "- Retrieve the data. (No cleaning needed yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='02'></a>\n",
    "## Part B. Feature engineering\n",
    "\n",
    "Mind you that choosing an algorithm and hyperparameter tuning might not be enough. If your data is of low quality, the algorithm will have poor performance as well. This is where feature engineering comes into play. Feature engineering involves transforming raw data into meaningful features that better represent the underlying problem to the predictive models, ultimately enhancing the model's performance. By selecting, creating, and refining features, you ensure that your data highlights the most relevant patterns and relationships, allowing the algorithm to learn more effectively. Proper feature engineering can often make the difference between a moderate model and a highly accurate one, even more so than the choice of algorithm itself. \n",
    "Possible modifications:\n",
    "- creation of new features derived from original features\n",
    "- selection of features\n",
    "- encoding features\n",
    "- log transformation \n",
    "- scaling\n",
    "- dimension reduction (*e.g.* PCA)\n",
    "\n",
    "### <span style=\"background-color: lightyellow;\">Feature engineering Task</span>\n",
    "- Review the [Study Case Feature Engineering notebook](../Study_Cases/study_case_feature_engineering.ipynb).\n",
    "- Review the [Study Case RNA-seq Preparation notebook](../Study_Cases/study_case_scanpy_object.ipynb).\n",
    "- Assess which data preparation and feature engineering steps could be beneficial for your dataset.\n",
    "- Implement a `DataProcessor` class tailored to your dataset and test it in the cell below. **Make sure it's an appropriate sklearn object, such as a `Transformer` or `Pipeline`.**\n",
    "- Update your repository with a new directory named `optimization`, including the following:\n",
    "    - The `DataProcessor` class as a Python file, complete with thorough documentation.\n",
    "    - An evaluation document that details and justifies your choices using a well-reasoned, argumentative approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE TO DEMONSTRATE THE USAGE OF THE PREPROCESSOR CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='03'></a>\n",
    "# Part C. Model Evaluation\n",
    "\n",
    "Evaluating the performance of a machine learning model goes beyond just looking at accuracy, as accuracy alone can be misleading, especially in cases where the dataset is imbalanced or where different types of errors have different consequences\n",
    "- **Detecting Overfitting/Underfitting**: A learning curve can help you understand whether your model is overfitting (performing well on training data but poorly on validation data) or underfitting (performing poorly on both training and validation data)\n",
    "- **ROC**: The ROC curve shows how well your model distinguishes between classes. It helps in selecting the optimal threshold for classification decisions, particularly when the cost of false positives and false negatives differs significantly. ROC curves are often used to compare models. \n",
    "- **Confusion matrix**: From the confusion matrix, you can derive other important metrics like precision, recall, F1-score, and specificity, which give a better understanding of how your model is performing across different classes\n",
    "\n",
    "\n",
    "### <span style=\"background-color: lightyellow;\">Evaluation Task</span>\n",
    "- Review the documentation for `sklearn.metrics` and `learning_curve`.\n",
    "- Select appropriate metrics for your dataset, including accuracy and indicators of overfitting or underfitting.\n",
    "- Implement functions to compute and assess these metrics. It is allowed to use libraries.\n",
    "- Add the evaluation functions as a Python module to your repository.\n",
    "- Update the evaluation documentation to clearly explain your choices for the evaluation metrics and the rationale behind them.\n",
    "\n",
    "See also: [Model evaluation video](https://video.hanze.nl/media/model-evaluation/0_gybpnhq7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE TO DEMONSTRATE THE USAGE OF THE EVALUATION MODULE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='04'></a>\n",
    "## Part D. Ensemble learning\n",
    "Ensemble learning is a powerful machine learning technique that combines the predictions of multiple models to improve overall performance, robustness, and accuracy. Rather than relying on a single model, ensemble learning methods aggregate the results of several models—often called \"weak learners\"—to produce a stronger predictive model. The key idea behind ensemble learning is that by combining models, the weaknesses of individual models can be offset, leading to better generalization on unseen data. Popular ensemble ML algorithms are the `Random Forest` and `XGBoost`. \n",
    "Here’s an improved version of the introduction:\n",
    "\n",
    "### voting algorithms\n",
    "Voting algorithms in ensemble learning combine the predictions of multiple classifiers to make a final decision, typically based on the consensus or weighted agreement among models. Two primary types of voting are commonly used: hard voting, where the final prediction is determined by the majority vote, and soft voting, which uses the weighted average of predicted probabilities to determine the outcome. Several sites explain the hard and soft voting algorithm. A clear explanation can be found on https://www.baeldung.com/cs/hard-vs-soft-voting-classifiers\n",
    "\n",
    "\n",
    "### <span style=\"background-color: lightyellow;\">Ensemble Task</span>\n",
    "- Review the [Study Case notebook on ensemble learning](..Study_Cases/study_case_bagging_boosting.ipynb).\n",
    "- Try three different algorithms for classification of your data label.\n",
    "- Implement a hard and soft voting algorithm for model aggregation.\n",
    "- Compare the performance of the voting algorithm with that of a boosting or bagging algorithm.\n",
    "- Update your repository with the voting algorithm class\n",
    "\n",
    "See also: [ensemle learning video](https://video.hanze.nl/media/Ensemble/0_sue5v33g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE WITH THE VOTING ALGORITHMS\n",
    "def vote_soft():\n",
    "    pass\n",
    "\n",
    "def vote_hard():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE TO COMPARE VOTING OUTCOME WITH A BAGGING OR BOOSTING ALGORITHM FROM SKLEARN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='05'></a>\n",
    "## Part E. Pipelines\n",
    "The sklearn pipeline function is a tool in machine learning that simplifies the workflow by encapsulating all the steps involved in a single object. It offers advantages such as simplicity, reproducibility, efficiency, flexibility, and integration. The Pipeline is built using a list of (key, value) pairs, where the key is a string containing the name you want to give this step and value is an estimator object (the method to be executed).\n",
    "\n",
    "### <span style=\"background-color: lightyellow;\">Pipeline Task</span>\n",
    "- Read the [Study Case notebook for a pipeline functions](..Study_Cases/study_case_pipeline.ipynb) to understand the principle of the pipeline function\n",
    "- Implement a pipeline which prepares and classifies data \n",
    "- Use a `GridSearchCV` object with the `Pipeline` object and a parameter grid to optimize choose the best hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE WITH THE PIPELINE IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"background-color: lightyellow;\">Bring it all together</span>\n",
    "\n",
    "By now, you've developed code snippets for model evaluation, optimization, and data improvement. Now, leverage these skills to build a classification model using the Clinical and Genetic Lung data. Make sure that you log your experiments. \n",
    "\n",
    "Once you're satisfied with the model, upload the relevant code to your repository and or refactor code with new insights. Furthermore, take a moment to reflect on its applicability and potential real-world impact. Update your evaluation document(s) with these findings in your repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: ML for operations\n",
    "\n",
    "If we intend to deploy the model in a real-world application, it's more efficient to save and reuse the trained model rather than retraining it each time. \n",
    "\n",
    "- Read the blog: https://neptune.ai/blog/saving-trained-model-in-python\n",
    "- Write three python files \n",
    "    1) a train_model python file \n",
    "    2) a use_model python file\n",
    "    3) a retrain_model python file that adds new data to the original training data and updates the model\n",
    "- Update your repository\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
