{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction Regression\n",
    "\n",
    "---\n",
    "\n",
    "Many advanced machine learning and data analysis techniques are founded on the principles of regression. A solid understanding of regression is often essential for mastering more complex algorithms, such as logistic regression, polynomial regression, Ridge Regression, Lasso, linear kernels in SVM, and neural networks. In this first assignment, we will explore the fundamentals of regression to establish a strong foundation for a wide range of machine learning applications.\n",
    "\n",
    "This notebook consists the following parts:\n",
    "\n",
    "- [A: data preparation regression](#01)\n",
    "- [B: Implementation of the cost function algorithm](#02)\n",
    "- [C: Implementation of the gradient descent algorithm](#03)\n",
    "- [D: Implementation of a regularization parameter](#04)\n",
    "- [E: Data preparation logistic regression](#051)\n",
    "- [F: Implementation of logistic regression](#05)\n",
    "- [G: Validate the outcome](#06)\n",
    "- [H: Development of the logistic regression class](#07)\n",
    "- [I: BONUS: Development of a polynomial class](#08)\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this two weeks you will be able to:\n",
    "- Understand the fundamental concepts of regression models.\n",
    "- Grasp the construction and significance of feature weights in modeling.\n",
    "- Understand model error, learning rates, and regularization factors and their impact on model performance.\n",
    "- Translate theoretical concepts into functional code, implementing machine learning models.\n",
    "- Articulate the appropriate use cases for different regression algorithms.\n",
    "- Build your own regression classes from scratch, implementing methods for fitting the model and making predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### Instructions\n",
    "- Ensure you fully understand the requirements and objectives of the assignment.\n",
    "- If you need additional context or clarification, please check the provided videos or background literature.\n",
    "- Work through each part of the assignment methodically, ensuring all tasks are completed.\n",
    "- Using the code from the tasks A to E, create **python files** with a classes that models data using (logistic) regression. These classes should include the following methods:\n",
    "  - `fit()`: A method to fit the model to the training data, determining the best weights.\n",
    "  - `predict()`: A method to predict the outcome for new input data using the trained model.\n",
    "  - `predict_proba()`: (optional) returns an array with probability per class for the specific sample.\n",
    "- Submit a repository with a directory `regression` that includes:\n",
    "  - This finished notebook and supportive code snippets\n",
    "  - The python file with the regression class \n",
    "  - The python file with the logistic regression class\n",
    "  - The python file with the polynomial class (optional)\n",
    "  - An evaluation document that details specific scenarios or datasets where each type of regression (linear, logistic, polynomial) is appropriate or inappropriate.\n",
    "\n",
    "\n",
    "### Additional Notes:\n",
    "- Do not add datafiles to your repository. Repositories with datafiles will not be accepted\n",
    "- Class solutions should be delivered in python files. Not in notebooks\n",
    "- When AI tools are used, you must provide proper references and explanations for how they were utilized. Failure to do so will be considered as academic fraud\n",
    "- The bonus assignment (H) is not mandatory\n",
    "- Use PEP8 \n",
    "\n",
    "Good luck!\n",
    "\n",
    "F.Feenstra\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import classification_report, mean_squared_error, r2_score, confusion_matrix\n",
    "from regression import Regr\n",
    "\n",
    "regr = Regr()\n",
    "\n",
    "def draw_costs(costs): \n",
    "    \"\"\" function to draw historical cost\"\"\"\n",
    "    plt.plot(range(1, num_iters + 1), J_history, color='b')\n",
    "    plt.xlabel('Number of iterations')\n",
    "    plt.ylabel('Cost (J)')\n",
    "    plt.title('Cost function over iterations')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def accuracy(y, y_pred):\n",
    "    \"\"\" function to calculate accuracy\"\"\"\n",
    "    match = 0\n",
    "    for i in np.arange(0,len(y)):\n",
    "        if y[i] == y_pred[i]:\n",
    "            match = match + 1\n",
    "    acc = match / len(y)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Linear Regression\n",
    "---\n",
    "## Use case regression\n",
    "The use case for the regression part in this notebook is the Delaney ESOL database. ESOL is a simple empirical model for aqueous solubility published by Delaney in 2004[1]. From molecule structures some molecure features were derived. The Delaney model (R2 0.74) published in the paper was: \n",
    "\n",
    "$$LogS = 0.16 -  0.63 cLogP - 0.0062 MW + 0.066 RB - 0.74 AP$$\n",
    "\n",
    "- $LogS$ = log of the aqueous solubility (the target)\n",
    "- $cLogP$ = octanol-water partition coefficient as calculated by the Daylight toolkit\n",
    "- $MW$ = molecular weight\n",
    "- $RB$ = number of rotatable bonds \n",
    "- $AP$ = aromatic proportion (number of aromatic atoms / total number of heavy atoms).  \n",
    "\n",
    "[1]. Delaney, John S. “ESOL: estimating aqueous solubility directly from molecular structure.” Journal of chemical information and computer sciences 44.3 (2004): 1000-1005.\n",
    "\n",
    "Note: It is possible to derive much more features using for instance the `deepchem featurizer` or the `rdkit`. The deepchem featurizer creates fingerprints to be used as features\n",
    "\n",
    "To load the data run the cell below. Consider several inspections to familiarize yourself with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MolLogP</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>NumRotatableBonds</th>\n",
       "      <th>AromaticProportion</th>\n",
       "      <th>logS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.5954</td>\n",
       "      <td>167.850</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.3765</td>\n",
       "      <td>133.405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MolLogP    MolWt  NumRotatableBonds  AromaticProportion  logS\n",
       "0   2.5954  167.850                0.0                 0.0 -2.18\n",
       "1   2.3765  133.405                0.0                 0.0 -2.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset for regression\n",
    "df_logS = pd.read_csv('https://raw.githubusercontent.com/dataprofessor/data/master/delaney_solubility_with_descriptors.csv')\n",
    "display(df_logS.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a name='01'></a>\n",
    "## Part A. Data preparation \n",
    "\n",
    "For this assignment you need to construct from the data a feature matrix $X$ containing a number of features ($n$ columns in the dataset except the class variable) and its observations (the number of rows $m$). Furthermore you need the column containing the class variabele as a vector $y$. Next we need a vector containing the weights per feature (the $\\theta$ vector). This vector we initialize with zero. Later on we will calculate the best weights, but first we create an algorithm that calculates the error with the given weights. Lastly we need to add a first column with ones for the $\\theta_0$ compution. \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "X = \n",
    "   \\begin{bmatrix} \\\n",
    "   1 & x_1^{(1)}  & x_2^{(1)} & .. & x_n^{(1)}\\\\\n",
    "   1 & x_1^{(2)}  & x_2^{(2)} & .. & x_n^{(2)}\\\\ \n",
    "   1 & x_1^{(3)}  & x_2^{(3)} & .. & x_n^{(3)} \\\\ \n",
    "   1 & .. & .. & .. & .. \\\\ \n",
    "   1 & x_1^{(m)}  & x_2^{(m)} & .. & x_n^{(m)} \\\\ \n",
    "   \\end{bmatrix} \n",
    "   \\\n",
    "   %\n",
    "   y = \n",
    "   \\begin{bmatrix} \\\n",
    "   y^{(1)} \\\\\n",
    "   y^{(2)} \\\\ \n",
    "   y^{(3)} \\\\ \n",
    "   .. \\\\ \n",
    "   y^{(m)} \\\\ \n",
    "   \\end{bmatrix} \n",
    "   %\n",
    "   \\\n",
    "   \\theta = \n",
    "   \\begin{bmatrix}\n",
    "    \\theta_0 & \\theta_1 & .. & \\theta_n \n",
    "  \\end{bmatrix}\n",
    "  %\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "See also Linear regression: https://video.hanze.nl/media/linear+regression/0_63duegik\n",
    "\n",
    "\n",
    "### <span style=\"background-color: lightyellow;\">Data preparation Task</span>\n",
    "- Slice the column, containing the class variable from the data to a vector $y$ \n",
    "- Slice the features columns into a matrix $X$. \n",
    "- add a column of 1's to the feature matrix $X$ (hint: use np.c_ and np.ones)\n",
    "- Create a $\\theta$ vector of size 1, $n + 1$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  1.        ,   2.5954    , 167.85      ,   0.        ,\n",
       "           0.        ],\n",
       "        [  1.        ,   2.3765    , 133.405     ,   0.        ,\n",
       "           0.        ],\n",
       "        [  1.        ,   2.5938    , 167.85      ,   1.        ,\n",
       "           0.        ],\n",
       "        ...,\n",
       "        [  1.        ,   3.6096    , 308.333     ,   4.        ,\n",
       "           0.69565217],\n",
       "        [  1.        ,   2.56214   , 354.815     ,   3.        ,\n",
       "           0.52173913],\n",
       "        [  1.        ,   2.02164   , 179.219     ,   1.        ,\n",
       "           0.46153846]], shape=(1144, 5)),\n",
       " 0      -2.180\n",
       " 1      -2.000\n",
       " 2      -1.740\n",
       " 3      -1.480\n",
       " 4      -3.040\n",
       "         ...  \n",
       " 1139    1.144\n",
       " 1140   -4.925\n",
       " 1141   -3.893\n",
       " 1142   -3.790\n",
       " 1143   -2.581\n",
       " Name: logS, Length: 1144, dtype: float64,\n",
       " array([ 0.16  , -0.63  , -0.0062,  0.066 , -0.74  ]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Your code here to create X, y and 𝜃\n",
    "\n",
    "def data_preparation(data):\n",
    "    ### begin solution ###\n",
    "    # LogS = 0.16 - 0.63*cLogP - 0.0062*MW + 0.066*RB - 0.74AP\n",
    "    theta = np.array([0.16, -0.63, -0.0062, 0.066, -0.74])\n",
    "    y = data[\"logS\"]\n",
    "    X = np.concatenate([np.ones((len(data),1)), data[data.columns[:-1]].to_numpy()], axis=1)\n",
    "    return X, y, theta\n",
    "\n",
    "    ### end solution ####\n",
    "\n",
    "data_preparation(df_logS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (1144, 5) matrix\n",
      "y (1144,) vector\n",
      "𝜃 (5,) vector\n",
      "There are 4 features,  and 1144 observations\n",
      "\n",
      "this should be the outcome:\n",
      "\n",
      "X (1144, 5) matrix\n",
      "y (1144,) vector\n",
      "𝜃 (5,) vector\n",
      "There are 4 features,  and 1144 observations\n"
     ]
    }
   ],
   "source": [
    "#execute this cell to validate the outcome\n",
    "\n",
    "try: \n",
    "    X, y, theta = regr.prep_data(df_logS, \"logS\")\n",
    "    m, n = X.shape\n",
    "    print(\"X\", X.shape, \"matrix\")\n",
    "    print(\"y\", y.shape, \"vector\")\n",
    "    print(\"𝜃\", theta.shape, \"vector\")\n",
    "    print (f\"There are {n-1} features,  and {m} observations\")\n",
    "except Exception as error:\n",
    "    print(f'This is not correct: create X, y, 𝜃 first. {error}')\n",
    "finally:\n",
    "    print('\\nthis should be the outcome:\\n')    \n",
    "    print('X (1144, 5) matrix')\n",
    "    print('y (1144,) vector')\n",
    "    print('𝜃 (5,) vector')\n",
    "    print('There are 4 features,  and 1144 observations')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='02'></a>\n",
    "## Part B. Implement cost function\n",
    "\n",
    "The cost $J(\\theta)$ (the error) of a linear regression model is computed using the equation:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} ( h_\\theta(x^{(i)}) - y^{(i)} ) ^2$$\n",
    "\n",
    "\n",
    "where \n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3 + ..\\theta_nx_n$$\n",
    "\n",
    "In which $J(\\theta)$ is the total cost calculated by the current weight values of $\\theta$; $h_\\theta(x)$ is the hypothesed value, the prediction, and $y$ is the actual value. $h_\\theta(x)$ is calculated for each observation $h_\\theta(x^{(i)})$ and compared to the actual value $y^{(i)}$. By adding up and eventually averaging the difference between these two values (hypotesis - actual) for each data observation, we arrive at the predictive value that the formula has with the current weight values of $\\theta$.\n",
    "\n",
    "\n",
    "\n",
    "To compute this we can use a naive loop or we can use the matrix computation functions included in Numpy, the so called vectorized implementation. \n",
    "\n",
    "### Naive loop implementation\n",
    "\n",
    "The naive loop implementation of calculating the error $J$ computes for each row $i$ the prediction $h$ which is substracted with the actual value ($h - y_i$) to get the difference between the actual value and the model value. The prediction is calculated using a for loop to compute the weight times the feature value for each feature according the equation $ h = \\theta_0 \\times 1 + \\theta_1 \\times x_1 + \\theta_2 \\times x_2 + ...\\theta_n \\times x_n$. The difference between the actual value and the model value is squared and averaged to estimate the average error of the model\n",
    "\n",
    "### Vectorized implemention\n",
    "\n",
    "For the hypothesis we can use a vectorized implementation: $ h_\\theta(x) = \\theta^T.X $\n",
    "\n",
    "\n",
    "See also \n",
    "https://video.hanze.nl/media/why_we_love_numpy.mov/0_npz2lx0a\n",
    "\n",
    "\n",
    "### <span style=\"background-color: lightyellow;\">Cost function Task</span>\n",
    "\n",
    "- implement the naive method to compute the cost\n",
    "- implement the vectorized method to compute the cost\n",
    "- use the method of preference to finish the compute cost function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive implementation\n",
      "0.11274283598399998\n",
      "0.026996461636000238\n",
      "0.5023464076960003\n",
      "0.15945486512400006\n",
      "0.0704201754239998\n",
      "0.09232603790400001\n",
      "0.00040920434944000637\n",
      "0.5838384182862401\n",
      "0.05680176089344009\n",
      "2.722898654592641\n",
      "0.08900150489343987\n",
      "0.05963308648128429\n",
      "0.22656254526641764\n",
      "1.6154776050073631\n",
      "1.5087975368934388\n",
      "2.6701102518278352\n",
      "0.08339893250027143\n",
      "0.02749001581461793\n",
      "0.3433794359330841\n",
      "5.708794630310558\n",
      "0.14510400799504006\n",
      "0.1426953647001602\n",
      "0.020239842382239946\n",
      "0.06243601638399993\n",
      "0.0017300941113599934\n",
      "0.05619175430400028\n",
      "0.01963170881423995\n",
      "0.2752488084505598\n",
      "1.066759291872161\n",
      "0.018141665481\n",
      "0.6580637191669363\n",
      "0.5244639984812846\n",
      "0.4569568919330839\n",
      "0.5501085062490002\n",
      "6.652831822310557\n",
      "0.8187310823737605\n",
      "0.11622999199504003\n",
      "0.023185178382240147\n",
      "0.0018833516857600032\n",
      "0.3884687339107599\n",
      "0.11574066676623987\n",
      "0.4720191981346834\n",
      "0.04965784387216017\n",
      "2.5577873583105593\n",
      "0.281808092736\n",
      "0.03574927999504008\n",
      "0.006042450382239922\n",
      "0.98931191245056\n",
      "0.13705307476623987\n",
      "0.28840823813468325\n",
      "1.7499067238721613\n",
      "0.7363479666918401\n",
      "1.1578546210680147\n",
      "1.3956511435782395\n",
      "0.5938120304640002\n",
      "0.6164630349825615\n",
      "0.16475367348196215\n",
      "0.3767970083989369\n",
      "0.27350494252900026\n",
      "0.19201398163600017\n",
      "2.883213508807835\n",
      "1.8083073791822373\n",
      "0.0806501651964376\n",
      "4.248843907001751\n",
      "0.7769780781249592\n",
      "0.004215177715359999\n",
      "0.43323592828355983\n",
      "0.67439966049856\n",
      "0.17304069153856014\n",
      "0.13235858924543983\n",
      "0.2180107378224398\n",
      "0.2229097585422396\n",
      "2.6959735313639963\n",
      "1.0581669065217592\n",
      "0.04339677546005946\n",
      "0.8197542302478386\n",
      "0.03199218740496004\n",
      "1.8807681309209967\n",
      "7.788765578895988\n",
      "4.031864005680032\n",
      "0.4453343116446021\n",
      "0.12362410704484021\n",
      "3.279819519139837\n",
      "1.9204150327321583\n",
      "9.866060666017942\n",
      "0.0023360402227600163\n",
      "2.4893190443577584\n",
      "0.26321024681603994\n",
      "1.0050479544326403\n",
      "0.5062763639610005\n",
      "3.102317891064034\n",
      "0.08530297317980129\n",
      "0.09502509072996022\n",
      "1.5737863076409597\n",
      "1.908392182578609\n",
      "0.10384372663156817\n",
      "2.009080968314945\n",
      "0.07583726540544455\n",
      "0.6244189996616891\n",
      "1.0127585832604973\n",
      "0.15356133727601545\n",
      "0.013355084321640053\n",
      "1.0366119103939573\n",
      "6.747351202408952\n",
      "4.336096563036156\n",
      "16.060599671596815\n",
      "0.4420545104352385\n",
      "5.081854062490237\n",
      "3.1295732835999996\n",
      "7.6052347020809785\n",
      "0.10558405396899995\n",
      "1.6756991821440008\n",
      "0.30167688070144005\n",
      "1.4421745803528698\n",
      "1.02308410316644\n",
      "6.322656942712344\n",
      "4.416181357263703\n",
      "0.7595289828921592\n",
      "0.8511506581256686\n",
      "0.7421987908921596\n",
      "2.2878930127923334\n",
      "1.9080272615449585\n",
      "0.054448572475905704\n",
      "1.5708666175285362\n",
      "0.3511490021256694\n",
      "0.5166154471521152\n",
      "0.8812696970344672\n",
      "0.11060830345900365\n",
      "0.3263992295449578\n",
      "4.861910340676\n",
      "2.1684867514590027\n",
      "2.6327602314590037\n",
      "0.45066194954495936\n",
      "0.6868425720932919\n",
      "5.3742533889409545\n",
      "5.576744593774234\n",
      "2.224016253544957\n",
      "0.03720439176335986\n",
      "2.679511669777958\n",
      "4.753232796480999\n",
      "0.0004155563790400087\n",
      "0.39961867971599974\n",
      "0.29423018091601166\n",
      "0.5366709167923365\n",
      "0.0007106394232741163\n",
      "0.09155360745900357\n",
      "1.7128518445638785\n",
      "2.0413541626815257\n",
      "1.826048445544958\n",
      "1.1130818939997653\n",
      "1.2210845903633993\n",
      "0.9594488516131581\n",
      "0.9015779756131577\n",
      "0.9028602759481598\n",
      "6.426359862707554\n",
      "0.697270029999765\n",
      "0.4893206756131583\n",
      "0.9028602759481598\n",
      "0.9607716519481594\n",
      "0.08062728241122588\n",
      "3.0426590071077375\n",
      "0.5919406603782394\n",
      "3.58611635457936\n",
      "1.2477706168013487\n",
      "5.093931636676\n",
      "6.29701006696336\n",
      "0.9206682676131572\n",
      "0.8826876836131581\n",
      "1.9886347079481594\n",
      "0.6880944716131577\n",
      "1.6388854119481597\n",
      "0.2898773596867604\n",
      "0.6543531842409996\n",
      "2.232998040441071\n",
      "0.4155179266062393\n",
      "0.06442652144643975\n",
      "0.08555075108835977\n",
      "5.0040526766759985\n",
      "1.3606381077674716\n",
      "6.29701006696336\n",
      "0.2500299393578498\n",
      "0.3051787786302981\n",
      "0.2729329854874407\n",
      "1.573323512441071\n",
      "1.6564630381346817\n",
      "1.1373453211008047\n",
      "6.653223882963362\n",
      "0.4624407172040038\n",
      "0.23273859463029703\n",
      "1.6587870394462065\n",
      "1.413277617036483\n",
      "0.11154913792371073\n",
      "0.11154913792371073\n",
      "0.019618932569759957\n",
      "0.926727752889002\n",
      "1.8546773677171604\n",
      "1.0165603998144401\n",
      "0.1048420363662399\n",
      "0.3073713351566396\n",
      "0.02923359046944441\n",
      "0.15119629434964194\n",
      "0.11366418873744016\n",
      "0.17499072491460416\n",
      "2.0186374043137603\n",
      "0.002902149286560014\n",
      "0.45113011090883953\n",
      "0.5504708087438399\n",
      "0.7357388366048381\n",
      "6.44696380920384\n",
      "9.706963359999285e-06\n",
      "0.1678170551630398\n",
      "0.40810657975683967\n",
      "0.05543990466624006\n",
      "0.09734150401599982\n",
      "0.6198944076446032\n",
      "0.3437278342600712\n",
      "0.0007081027440400074\n",
      "0.036885737603560134\n",
      "0.1344391156083598\n",
      "0.3310002364064399\n",
      "0.1908778765091656\n",
      "6.65561526084435\n",
      "0.0018722409763599819\n",
      "0.5553513579609974\n",
      "0.30902599592113744\n",
      "1.086781230144\n",
      "0.5359809522918396\n",
      "1.634531566144\n",
      "1.7207080011577587\n",
      "0.018528708848040157\n",
      "0.2594779683440399\n",
      "0.6933216764083593\n",
      "0.28289250487696005\n",
      "6.387854552314942\n",
      "1.454264753040999\n",
      "2.7743959188432408\n",
      "0.18503246371599985\n",
      "7.647772330152355\n",
      "0.15385843644975006\n",
      "3.8373730513779596\n",
      "0.0032358349633599937\n",
      "1.980832557648279\n",
      "2.3008600123939598\n",
      "0.53791017703696\n",
      "1.6961993948768395\n",
      "1.3115251836985602\n",
      "3.089349245498186\n",
      "1.5147943005920406\n",
      "0.5331947749344162\n",
      "0.016751555412839965\n",
      "0.4666931027203582\n",
      "0.1636061737027594\n",
      "0.08992789444803961\n",
      "0.019941111368999884\n",
      "0.489900605041\n",
      "0.8438182437776397\n",
      "0.8262450039920396\n",
      "0.7858258696112399\n",
      "1.8834521162766404\n",
      "0.122282116969959\n",
      "0.6382162845633599\n",
      "0.5533520984769602\n",
      "0.5814721318662398\n",
      "4.478264348481001\n",
      "1.559804759040583e-05\n",
      "3.219591200441071\n",
      "0.9534818131008049\n",
      "6.197034690963361\n",
      "2.763673124916009\n",
      "2.904713252441071\n",
      "0.9148232557674707\n",
      "6.297010066963356\n",
      "1.8157011065725193\n",
      "0.022207457136111095\n",
      "0.5312072746573332\n",
      "3.171204476313761\n",
      "0.5723669946403596\n",
      "1.566199186849438\n",
      "0.5466972763440403\n",
      "0.0028504707440399927\n",
      "3.2855909143768374\n",
      "0.20853780760835994\n",
      "0.29738066440643995\n",
      "2.733778989165713\n",
      "0.2505210712326401\n",
      "1.8749030714410004\n",
      "0.6164542413160002\n",
      "0.9289351134438396\n",
      "0.19970566696336006\n",
      "0.20874335496336008\n",
      "0.46313904706623954\n",
      "0.06559755684803978\n",
      "0.37135592834403996\n",
      "0.9267131204083593\n",
      "0.15222014371599982\n",
      "5.213732367689149\n",
      "8.096646026152355\n",
      "0.21103476197904025\n",
      "3.6059024833779594\n",
      "2.12243698039396\n",
      "0.036054338448040145\n",
      "3.1616268076929503\n",
      "0.5623935050410003\n",
      "0.6865709997776396\n",
      "0.49055835452610513\n",
      "2.0636713055425586\n",
      "3.3577992860588655\n",
      "3.0106303096422398\n",
      "7.112889000005789e-06\n",
      "0.022207457136111095\n",
      "3.171204476313761\n",
      "0.05366737490883986\n",
      "0.23647122092673775\n",
      "0.015225042744040053\n",
      "0.11700271760356018\n",
      "0.22148286168926082\n",
      "6.655615260844348\n",
      "0.5703557379609978\n",
      "0.30902599592113744\n",
      "0.18651722487695996\n",
      "0.37276299906623966\n",
      "0.6605929221872875\n",
      "0.0007227709633600076\n",
      "1.9927683503709142\n",
      "0.2991378269099274\n",
      "0.38603580859684006\n",
      "0.2749691406250005\n",
      "0.3874161172417605\n",
      "0.8113048050451603\n",
      "1.7743630635248415\n",
      "1.7567058363008408\n",
      "0.36389531688695076\n",
      "1.648469054899833\n",
      "0.3034200785616405\n",
      "0.16231906285456046\n",
      "0.1321803817824402\n",
      "0.1764742078027782\n",
      "0.8696286135054414\n",
      "0.9139352352001612\n",
      "0.2224912008325439\n",
      "0.19336604938720456\n",
      "0.009058751314750701\n",
      "0.29667347007076056\n",
      "1.2324254033770332\n",
      "0.3166615264684849\n",
      "0.1636061737027594\n",
      "1.372504960376184\n",
      "0.2257936305874777\n",
      "0.2776247340544709\n",
      "1.188419540640192\n",
      "1.67424759889983\n",
      "0.5713864225878041\n",
      "1.0478684795345503\n",
      "0.04630134476929615\n",
      "3.6073982289510305\n",
      "0.44218607618987066\n",
      "2.1240746109816118\n",
      "0.08354487952882812\n",
      "1.866029582592069\n",
      "0.21701634672668363\n",
      "4.4051620875648405\n",
      "2.1489495921561605\n",
      "0.5812220445926397\n",
      "2.3371420404970804\n",
      "0.473829852609\n",
      "0.8509426601879507\n",
      "0.2441923266272389\n",
      "0.02421646394896041\n",
      "0.8565794960496398\n",
      "0.5737520142816399\n",
      "0.667621360561\n",
      "0.07856786576016087\n",
      "0.00022123389280442066\n",
      "0.6567789555609993\n",
      "0.781999223971839\n",
      "4.92373288944196\n",
      "0.6272145801734399\n",
      "1.2972918417638402\n",
      "3.3805271816410003\n",
      "12.096865191803031\n",
      "4.3196842082249995\n",
      "0.4379562096627606\n",
      "0.1407932002656391\n",
      "0.0031502176782399415\n",
      "4.989156268826312\n",
      "1.3794757759180842\n",
      "0.36229858104920537\n",
      "1.2403768417802707\n",
      "0.5513638695054403\n",
      "2.639832926072604\n",
      "0.14368073790586083\n",
      "0.2508356972250017\n",
      "0.07128665041935929\n",
      "0.1449048040495863\n",
      "2.0765660610425214\n",
      "0.02797509520018424\n",
      "0.04728189313599979\n",
      "4.619744970624633\n",
      "3.8142121248006395\n",
      "10.06563931208297\n",
      "0.21667053017594232\n",
      "0.01926980924462121\n",
      "0.06491447900224027\n",
      "2.2500556289152667\n",
      "0.9980797227523596\n",
      "0.15496275189237824\n",
      "4.877269070115998\n",
      "0.6592046681616399\n",
      "2.102346302809001\n",
      "0.0003135480797945858\n",
      "1.6048889614124948\n",
      "0.01901099682528009\n",
      "0.08342253783616033\n",
      "2.4625221583127517\n",
      "0.00020823293068614564\n",
      "0.5496766290432397\n",
      "0.3060967751360176\n",
      "0.23703733347904\n",
      "0.4604848146423855\n",
      "0.31377849444421607\n",
      "4.095138324182202\n",
      "8.644804265943025\n",
      "6.411980118845432\n",
      "11.449052212182197\n",
      "4.16651703554303\n",
      "5.300073270845431\n",
      "7.796318294845429\n",
      "7.496306682194555\n",
      "0.3637834085091603\n",
      "0.09166640301316009\n",
      "1.03242913365904\n",
      "0.03615706596287037\n",
      "1.8628242575502394\n",
      "1.3043110937473594\n",
      "1.7163006978675592\n",
      "0.06342096795903997\n",
      "0.14767895781604046\n",
      "0.3985972207233174\n",
      "0.000399560120999886\n",
      "1.0535607737206052\n",
      "0.5808232362297571\n",
      "0.24343408681215825\n",
      "0.02008637245696001\n",
      "0.013351399728751854\n",
      "11.469721988524865\n",
      "0.19419949194336797\n",
      "0.5566186543334398\n",
      "0.44290994901903935\n",
      "0.09507676236303986\n",
      "0.07596351797904002\n",
      "1.0744151715999992\n",
      "0.22601786632404272\n",
      "0.15015916683234712\n",
      "0.013819507180959948\n",
      "2.4825576762946127\n",
      "0.8672623228686412\n",
      "0.006317005142905084\n",
      "0.03237150455382838\n",
      "0.017099432919040033\n",
      "0.34638044141917496\n",
      "1.4237228990419597\n",
      "1.6837636838406402\n",
      "0.6914700805902406\n",
      "0.03150135119044013\n",
      "0.12260730466473775\n",
      "0.44481643335935883\n",
      "0.3094590091321602\n",
      "0.4018943775619588\n",
      "1.5213436444417583\n",
      "0.5865498109171601\n",
      "2.2485751093062934e-05\n",
      "0.11352883315216032\n",
      "8.697800643999702e-05\n",
      "1.892340839126439\n",
      "0.27798243939216155\n",
      "0.009512426002777716\n",
      "2.4367172536014374\n",
      "0.33717494366636724\n",
      "0.0017794729824400087\n",
      "0.40541150944044146\n",
      "0.0020241360921599686\n",
      "0.7275213142419606\n",
      "0.6530889078028822\n",
      "0.045261371107840155\n",
      "0.2545678770278395\n",
      "0.01718416861455997\n",
      "2.926473035848734\n",
      "0.016799892687359746\n",
      "0.3685847376992402\n",
      "0.8085383399937606\n",
      "0.2848600272398386\n",
      "0.04706611428903793\n",
      "2.6408710668432835\n",
      "0.022920627699359982\n",
      "0.4730949767610001\n",
      "0.07094312916229303\n",
      "0.4616577381160003\n",
      "0.001266932836000009\n",
      "0.43589731471503995\n",
      "0.0022002344843176543\n",
      "0.019229646240999997\n",
      "0.03487741644282002\n",
      "1.655719239048896\n",
      "1.4875966881838243\n",
      "0.10422020506352257\n",
      "0.11498609721599853\n",
      "0.6943324489602807\n",
      "0.5284135171660401\n",
      "0.4312959168157093\n",
      "0.11959945603495117\n",
      "0.04678355062082152\n",
      "6.065963104389747\n",
      "7.949417847629434\n",
      "0.12976478834944002\n",
      "3.9193023648398375\n",
      "1.1848061261440002\n",
      "0.0745341324883604\n",
      "0.6349737473857602\n",
      "0.23408814368017292\n",
      "6.404300771583996\n",
      "0.08768492234896096\n",
      "0.007458740496000095\n",
      "0.21105074888675776\n",
      "0.9192074428427359\n",
      "0.3145533547312238\n",
      "3.181716323042715\n",
      "0.5647793043240005\n",
      "0.04362776303075938\n",
      "0.04415201132644013\n",
      "1.1516650881235597\n",
      "1.7979784831238377\n",
      "0.5474676959923598\n",
      "1.35607515564096\n",
      "2.521634111597161\n",
      "1.5967294390297597\n",
      "0.7177142912720399\n",
      "0.6493893874090001\n",
      "0.8230786112742395\n",
      "0.832706255750761\n",
      "3.285866063337641\n",
      "2.7164208373862375\n",
      "0.43909309488100035\n",
      "1.1432634852250008\n",
      "1.2912867771039997\n",
      "0.5624508010758402\n",
      "2.0575785232464407\n",
      "2.6793990507690006\n",
      "0.07780584996899975\n",
      "0.836145471524281\n",
      "2.352339329682205\n",
      "3.17782299229414\n",
      "0.2297593579568402\n",
      "0.5890553290003575\n",
      "0.0693530428024693\n",
      "1.809324857758148\n",
      "0.6864183256801075\n",
      "0.46870780175555204\n",
      "5.625156319777434\n",
      "0.005701518470560726\n",
      "2.015737036216517\n",
      "0.40362972350596305\n",
      "9.124070400005523e-05\n",
      "0.12595389385093544\n",
      "0.06472627986495927\n",
      "1.0204640364720368\n",
      "0.1997557211521603\n",
      "1.9913210990705514\n",
      "3.8017033565122067\n",
      "3.8017033565122067\n",
      "0.11549326464899887\n",
      "0.046793996288640595\n",
      "0.01369841088004452\n",
      "0.5155397961209971\n",
      "0.010657341343359782\n",
      "0.7056134400639994\n",
      "0.0017179532832400407\n",
      "0.005809000602239471\n",
      "0.760367257692157\n",
      "0.5075811971859535\n",
      "0.3995878311961599\n",
      "0.2144082097221876\n",
      "0.09654878272900073\n",
      "1.6003702252947574\n",
      "0.09721549843599939\n",
      "0.09357236281599929\n",
      "0.5395293261075602\n",
      "0.02775156174399984\n",
      "0.0012678014784399945\n",
      "0.25322271951375935\n",
      "0.4301066929550432\n",
      "0.01686379027357451\n",
      "2.9152846174699194\n",
      "0.7629830331210035\n",
      "2.360177131854243\n",
      "2.515614238333425\n",
      "0.092011515556\n",
      "1.184052514727697\n",
      "0.1505218072179597\n",
      "0.04038050060100002\n",
      "10.252382621129641\n",
      "0.1278212343522187\n",
      "2.1524006020784396\n",
      "0.015907566075039816\n",
      "0.6543353880999998\n",
      "0.13160211435943153\n",
      "0.15377801474115993\n",
      "0.0032118515982400356\n",
      "3.616269749011519\n",
      "0.11462797197198511\n",
      "1.2295057430286416\n",
      "0.45015109048899943\n",
      "0.23369898476988699\n",
      "2.0700015624999866\n",
      "0.5085680890881589\n",
      "0.22120648356587222\n",
      "0.10362468122512188\n",
      "0.009469298977335262\n",
      "0.33765312606944475\n",
      "0.06865060255876002\n",
      "1.3549462852665577\n",
      "0.8233990784995588\n",
      "0.015691868758635704\n",
      "0.10169883184075096\n",
      "2.4233690634627547\n",
      "0.5674552510254379\n",
      "0.005831023467142267\n",
      "0.0005794708128400302\n",
      "6.9269649868038226\n",
      "1.334157597294757\n",
      "0.0006247800193599337\n",
      "0.7136025624999961\n",
      "0.7987354383999985\n",
      "1.4437651779968363\n",
      "0.05769453018130616\n",
      "0.12559368966399972\n",
      "1.0108259427225579\n",
      "0.18366507616267716\n",
      "2.2206126747366866\n",
      "0.03544499043856071\n",
      "3.7965015223133887\n",
      "0.4720514436000002\n",
      "0.046221732057759986\n",
      "1.50000227461764\n",
      "0.020880775457850758\n",
      "0.7283215999897611\n",
      "0.9117224810905549\n",
      "0.5733061060320404\n",
      "2.0642603324115565\n",
      "0.2901588935557051\n",
      "0.2901588935557051\n",
      "1.7015355378126398\n",
      "0.4082944180326401\n",
      "0.010688920642910408\n",
      "0.2170635554406402\n",
      "0.24830442323313576\n",
      "1.656826173941757\n",
      "0.18511402990144\n",
      "0.5290653712134398\n",
      "0.3649358434406398\n",
      "1.5722852724639966\n",
      "1.2335176555545602\n",
      "0.07334867223616007\n",
      "0.08162997553215993\n",
      "0.1859826000490001\n",
      "0.001591642941160002\n",
      "0.13387432125455984\n",
      "3.6287956416240355\n",
      "0.014237453312639975\n",
      "0.0075941813091600415\n",
      "0.06416180188324001\n",
      "0.2012039205177599\n",
      "6.399131724425958\n",
      "2.2819787597452423\n",
      "0.4132316578278002\n",
      "0.7834933549825618\n",
      "0.6585708779664411\n",
      "0.23376736228009362\n",
      "0.00015219498933774942\n",
      "0.6183877314591203\n",
      "0.35628498930674435\n",
      "0.6078529317623997\n",
      "0.47270869183915487\n",
      "1.4153714963842874\n",
      "0.6093682088961602\n",
      "0.46724415696675947\n",
      "0.37288978081434476\n",
      "0.0633168699868071\n",
      "0.025226291229884564\n",
      "0.03339456546723986\n",
      "4.833998501587357\n",
      "1.0803572303369133\n",
      "0.03080797248399983\n",
      "1.7931266762982352\n",
      "1.943031023067178\n",
      "0.17667017537143584\n",
      "0.02226642933635967\n",
      "0.004293132483999894\n",
      "0.7136755507662411\n",
      "0.0019261399127938979\n",
      "0.04668424922023693\n",
      "0.28413822087465423\n",
      "0.0005847691240000072\n",
      "0.17584301636496014\n",
      "0.31753991364624046\n",
      "0.9765649333689994\n",
      "2.2091513890306116\n",
      "0.3189061936368399\n",
      "0.14506331631610692\n",
      "0.1588140458190404\n",
      "0.012385820230559886\n",
      "0.12115954716804012\n",
      "0.08455300839999982\n",
      "0.2901578275690006\n",
      "0.0016403202007511445\n",
      "7.218107859482449\n",
      "0.8725749433358404\n",
      "0.0016904746220548523\n",
      "0.7400555261190407\n",
      "6.038854078463996\n",
      "0.28314488330495974\n",
      "0.00020529158399999704\n",
      "1.424514815724167\n",
      "15.420028405023336\n",
      "3.1880216772102368\n",
      "4.137719030354558\n",
      "0.04621743230976125\n",
      "3.7016482475022316\n",
      "0.46140583094596\n",
      "0.21875649270207137\n",
      "1.3023483955430408\n",
      "0.9411396623183387\n",
      "0.00015135888783997958\n",
      "1.5576772272163553\n",
      "0.8152735556250011\n",
      "0.17985538156096148\n",
      "1.3665310737638396\n",
      "0.3642338153858848\n",
      "4.573005375999927e-05\n",
      "0.018793339085439974\n",
      "1.327364825614238\n",
      "1.1546936643866705\n",
      "0.4598357275075502\n",
      "0.02979204218522461\n",
      "0.013226886067239976\n",
      "1.889571896688997\n",
      "0.14613631637283983\n",
      "0.24142459022550986\n",
      "1.2224720945011607\n",
      "0.014479800920179434\n",
      "0.00049920964900001\n",
      "0.04887344818756004\n",
      "1.7376791859523588\n",
      "1.11504434108356\n",
      "0.006287676501604976\n",
      "0.0077876421147635684\n",
      "5.0395969079998455\n",
      "0.77245360856356\n",
      "0.21679509327876004\n",
      "0.21561442164900016\n",
      "1.2091993324960006\n",
      "0.0022057997884603543\n",
      "5.658920748225533\n",
      "0.21224596424255973\n",
      "0.0003250376294400024\n",
      "0.2373225921921106\n",
      "0.17960915233023966\n",
      "1.7921937289665604\n",
      "0.169841245924\n",
      "0.22823627198464072\n",
      "0.2834793622128407\n",
      "0.37414188870066756\n",
      "3.786894983229143\n",
      "0.2171960778490002\n",
      "4.587970062849425\n",
      "0.05032960217476048\n",
      "0.4912181962041586\n",
      "0.020147985580959954\n",
      "0.2893221481414665\n",
      "1.9870950790209472\n",
      "0.5461345976846399\n",
      "0.004322904701439978\n",
      "1.8063214848291487\n",
      "2.4315275134273615\n",
      "1.0546156222470398\n",
      "0.10666494721600021\n",
      "0.03186903336100019\n",
      "0.025213247854239756\n",
      "0.0009072713550756765\n",
      "0.24434830785600173\n",
      "0.19287345093696015\n",
      "0.003439306399359501\n",
      "0.6651220821171714\n",
      "0.018918792079360043\n",
      "0.1410466656192372\n",
      "0.2636103017086193\n",
      "0.06262366110783955\n",
      "0.0023444576641600758\n",
      "0.15232581575235993\n",
      "7.140926016000135e-05\n",
      "1.0502507033427606\n",
      "0.31336529193216\n",
      "1.85508594983716\n",
      "0.8383678491123414\n",
      "0.9552982493209602\n",
      "0.00019525031823999122\n",
      "0.30239998591521655\n",
      "0.8027737093569542\n",
      "0.26371237619377796\n",
      "0.11781754191936025\n",
      "2.1561317692943787\n",
      "0.7608179691014398\n",
      "0.09324472959999991\n",
      "0.019668940515999902\n",
      "0.010252980049000032\n",
      "0.1540091536\n",
      "4.596341512463997\n",
      "0.35226409113856\n",
      "0.15128199479078816\n",
      "0.15274777623616018\n",
      "0.7845310960537603\n",
      "2.5935401839878365\n",
      "1.3345369451065594\n",
      "1.2114048875545587\n",
      "1.339502539213439\n",
      "0.02177395360000004\n",
      "0.2904985780326401\n",
      "0.1888133661075599\n",
      "1.098825966001\n",
      "3.1388152975680383\n",
      "2.4161612252803577\n",
      "0.598275951289003\n",
      "0.001339864795517687\n",
      "0.10553921742399794\n",
      "0.5690571488468541\n",
      "0.4908874561991415\n",
      "0.07624004545600026\n",
      "0.038763058876768494\n",
      "0.15707968835583977\n",
      "0.036300486921817844\n",
      "0.359027868593575\n",
      "0.022658337588484487\n",
      "0.1940984003560004\n",
      "0.4952224013761608\n",
      "0.11756340277504057\n",
      "1.4472470150496395\n",
      "0.1399573942374401\n",
      "0.7916265907559996\n",
      "0.029830747569639916\n",
      "0.07617503794589497\n",
      "0.04181501232529511\n",
      "9.424406917300774e-05\n",
      "0.12637841433393854\n",
      "0.09959780551743977\n",
      "0.21476758397605514\n",
      "2.4015434352339597\n",
      "0.0411302465952043\n",
      "3.576322751242241\n",
      "0.033122908009000473\n",
      "11.306212570829427\n",
      "0.19031161950783995\n",
      "0.29667382891715377\n",
      "0.059853328920359626\n",
      "0.3448742292986708\n",
      "0.12204779809156034\n",
      "0.0307985852796378\n",
      "3.056328179303536\n",
      "2.215543536288035\n",
      "0.3029599346936017\n",
      "0.11296189004749162\n",
      "2.4138907352434376\n",
      "5.403399333850765e-05\n",
      "0.025326546532190474\n",
      "0.17568978829623144\n",
      "0.055387520059950936\n",
      "0.03908742518915993\n",
      "0.3673597152176024\n",
      "3.9898550054332915\n",
      "4.591317565696001\n",
      "0.16890045062499998\n",
      "0.6435187432944397\n",
      "10.71873026838783\n",
      "3.6424058984007974\n",
      "0.49941443089476034\n",
      "0.12620284670015752\n",
      "1.2948232374089983\n",
      "0.02402741806083984\n",
      "0.429171033758149\n",
      "0.9321238247833599\n",
      "0.76903218224704\n",
      "0.12012878721600025\n",
      "0.035539413361000124\n",
      "0.003747055854240064\n",
      "8.413930539141754\n",
      "1.5385693367246402\n",
      "0.16856444035600018\n",
      "0.19567476108196036\n",
      "0.0016403202007511445\n",
      "0.020767346237440042\n",
      "0.012371615403546634\n",
      "0.025514950756000014\n",
      "0.003933271569639985\n",
      "5.703067087821097\n",
      "0.43665913635912174\n",
      "0.02464403904964004\n",
      "0.7865717435654359\n",
      "1.7080456368528396\n",
      "0.17153794857795987\n",
      "0.02279250452481763\n",
      "0.08737415751743977\n",
      "0.13104255200400083\n",
      "0.19090442934481275\n",
      "1.809324857758148\n",
      "0.6864183256801075\n",
      "0.7840493287833598\n",
      "0.7935531458560001\n",
      "1.3658534569404401\n",
      "0.19090442934481275\n",
      "0.0037766231670376556\n",
      "0.2373168762432395\n",
      "0.02748414777253579\n",
      "0.5430876382470401\n",
      "0.008724307215999925\n",
      "0.09089079336099994\n",
      "0.09534928785423948\n",
      "1.9792511203939602\n",
      "0.49156336500963715\n",
      "0.31873413996980277\n",
      "0.5380132079834462\n",
      "0.23543650752400014\n",
      "0.2668668811905599\n",
      "0.6450319904755343\n",
      "2.6924504188134395\n",
      "0.4416118167054413\n",
      "0.16878096023616002\n",
      "0.5941782722361602\n",
      "2.518762098319238\n",
      "10.676419015440993\n",
      "0.1975214025824405\n",
      "0.05201124430714397\n",
      "9.273272273943027\n",
      "0.2220167777514448\n",
      "0.001038596386268418\n",
      "1.1222419146246367\n",
      "1.0933136559489611\n",
      "0.0033053607264711318\n",
      "0.26353667884238136\n",
      "0.2224912008325439\n",
      "3.7437308725893557\n",
      "2.494809509411563\n",
      "0.12075142781737545\n",
      "0.14773503609135596\n",
      "2.829052683649439\n",
      "1.8014337306249992\n",
      "0.05631100524035997\n",
      "1.5442808691571714\n",
      "0.6974852102736376\n",
      "0.34651464175014374\n",
      "1.8685566479646307\n",
      "4.693809777008039\n",
      "1.1988345571990378\n",
      "0.16055600219136026\n",
      "0.8756044137051315\n",
      "0.6780813210547373\n",
      "2.545885748122232\n",
      "0.002579743095086592\n",
      "0.06349138229954672\n",
      "0.004547209488999681\n",
      "0.07026733779107548\n",
      "1.0312289647352233\n",
      "0.20791867236099976\n",
      "0.7106756390496398\n",
      "0.3070365622374403\n",
      "0.11476662813082006\n",
      "1.3449829507559996\n",
      "0.04960232756964001\n",
      "0.001625611836338151\n",
      "0.002287881091239828\n",
      "0.06549607008399953\n",
      "0.7951085593699587\n",
      "0.0047822757159996536\n",
      "0.9262826691216411\n",
      "1.03632318560016\n",
      "0.901409602885868\n",
      "0.078127405363842\n",
      "0.11260105049664036\n",
      "1.9840418370240391\n",
      "0.2225171706863817\n",
      "0.8729718014361597\n",
      "0.25709976490615843\n",
      "3.1309143747209993\n",
      "0.6600087082446404\n",
      "0.6568181804262442\n",
      "1.1862150482249993\n",
      "1.0838546457555596\n",
      "0.32067190584100025\n",
      "0.05094482353215998\n",
      "0.94924178353216\n",
      "0.003246629232640001\n",
      "0.01627155360000003\n",
      "0.5788541734084445\n",
      "6.056224699224037\n",
      "0.05196065542144003\n",
      "0.006885580824359951\n",
      "0.006607738944000004\n",
      "0.21115639129030003\n",
      "2.8346948324870387\n",
      "4.2894552099999865\n",
      "0.009229221320263188\n",
      "0.0705386855174399\n",
      "0.22996494157156078\n",
      "0.009256620661777711\n",
      "0.04532027868735996\n",
      "2.114385871011837\n",
      "6.1346174978559995\n",
      "6.309342195072158\n",
      "6.1346174978559995\n",
      "1.0057607170987166\n",
      "0.970769847137351\n",
      "2.21631224896656\n",
      "1.7521446265959986\n",
      "0.6969577301897195\n",
      "7.510156089528922\n",
      "0.17615497314724007\n",
      "0.35937989253039027\n",
      "1.03952216924416\n",
      "6.47016191714982\n",
      "0.04035106230246842\n",
      "1.0218565395086385\n",
      "0.0940701018570198\n",
      "0.3541573743717952\n",
      "0.12583554302229738\n",
      "0.032235394021168595\n",
      "0.08566415800335933\n",
      "0.030144792133145023\n",
      "0.8057177148768374\n",
      "0.9558119952964801\n",
      "0.45512830701844115\n",
      "0.6075706419915241\n",
      "0.36342209402500036\n",
      "1.0411104062812098\n",
      "0.8438649758982555\n",
      "1.6859180164328866\n",
      "1.5439751823053462\n",
      "0.10918624574854545\n",
      "0.07317426484454602\n",
      "0.18398166848697228\n",
      "0.059366209902263996\n",
      "0.06250106667121832\n",
      "0.09720427417600024\n",
      "0.21008600588196039\n",
      "0.11485443004324025\n",
      "0.03942400860303988\n",
      "0.2677313212417609\n",
      "0.32116263762241626\n",
      "6.286780646304808\n",
      "0.8784753206914497\n",
      "0.28019617742736047\n",
      "0.8251758834809086\n",
      "1.5394943110849715\n",
      "0.01294043403634331\n",
      "0.3696586496193586\n",
      "1.0395964167013905\n",
      "0.14258901001215998\n",
      "0.27013645891201754\n",
      "0.15275731262723996\n",
      "0.9545028165795514\n",
      "0.9780182314709738\n",
      "0.22489712798223918\n",
      "0.021744993183736013\n",
      "0.19727632896400005\n",
      "0.4406843422083593\n",
      "1.0165603998144401\n",
      "3.9869490628371524\n",
      "0.03758206516546044\n",
      "0.5620545882089963\n",
      "1.0599789679878395\n",
      "1.5414518653248404\n",
      "0.002721521091240243\n",
      "0.3109159756825578\n",
      "0.2274030894239971\n",
      "0.5742402280249588\n",
      "0.06819242831424004\n",
      "0.061756356052868715\n",
      "0.008469447276160008\n",
      "16.162773495143007\n",
      "2.210412941662458\n",
      "1.6359622514467602\n",
      "1.1919787645728401\n",
      "2.70558021027904\n",
      "1.024057353327479\n",
      "2.919546018777609\n",
      "0.3313935399468399\n",
      "0.0012429170395722151\n",
      "2.010494290561001\n",
      "0.051277156869160095\n",
      "0.49466099169796013\n",
      "0.09112803262563993\n",
      "2.047847433312039\n",
      "0.7621108417081591\n",
      "0.07943768309237866\n",
      "1.095193226619041\n",
      "0.014604674160039974\n",
      "1.6152966176262402\n",
      "0.6187915815734047\n",
      "0.44640994198470124\n",
      "0.008541921146893094\n",
      "0.9797908203870791\n",
      "4.621308116839836\n",
      "3.3673248247398377\n",
      "1.1848061261440002\n",
      "0.026433492022440486\n",
      "0.13205156532099852\n",
      "1.289503781194239\n",
      "0.14763361505343855\n",
      "0.2276209748929589\n",
      "0.007826533012051124\n",
      "1.0527160589898001\n",
      "1.0886693698062397\n",
      "0.01621404942336002\n",
      "5.66190793815696\n",
      "0.01621404942336002\n",
      "0.06164345496099999\n",
      "0.07326917821584018\n",
      "0.4376822745153599\n",
      "0.2660844363988495\n",
      "0.6507535013439466\n",
      "0.02956827864304048\n",
      "1.092878143795216\n",
      "2.869219291129\n",
      "5.721124377523843\n",
      "2.0074415979342315\n",
      "0.2945007141126393\n",
      "0.20876594629741344\n",
      "2.215546513229435\n",
      "1.2252946541023977\n",
      "0.9766151352345606\n",
      "0.31062921321743997\n",
      "13.00353602093056\n",
      "0.037093142177639996\n",
      "12.180746356574758\n",
      "1.0836966844567477\n",
      "0.14706857509252355\n",
      "0.002713176049833324\n",
      "0.006507735775674561\n",
      "Error: 0.599\n",
      "expected Error with theta initialized with zeros: 6.871\n",
      "Execution time 1.57e+07 nano seconds\n"
     ]
    }
   ],
   "source": [
    "#Pseudo code for naive implementation to compute j_val_nav\n",
    "\n",
    "print (\"Naive implementation\")\n",
    "start_time = time.time_ns()\n",
    "\n",
    "# REPLACE THE PSEUDO CODE WITH YOUR CODE\n",
    "\n",
    "#Initialize the cost variable to hold the cumulative error value\n",
    "#Loop over each observation i from first observation to mth observation:\n",
    "    # Get the i-th feature row (example)\n",
    "    # Initialize the prediction value for the current example with zero\n",
    "    # Loop over each feature and its corresponding weight in theta\n",
    "        # add the the product of each weight and feature to the prediction value\n",
    "    #Calculate the squared difference between the predicted value and actual value\n",
    "    #Add the difference to accumulated error value\n",
    "#Calculate the mean squared error (J_val_nav) by dividing the accumulated error by twice the number of examples\n",
    "\n",
    "### begin solution ####\n",
    "err = 0\n",
    "for i in range(0,len(X)):\n",
    "    y_hat = sum(theta * X[i])\n",
    "    se = (y_hat - y[i])**2\n",
    "    print(se)\n",
    "    err += se\n",
    "    \n",
    "mse = err / (len(y) * 2)\n",
    "J_val_nav = mse\n",
    "### end solution ######\n",
    "\n",
    "\n",
    "\n",
    "end_time = time.time_ns()\n",
    "try: \n",
    "    print(f\"Error: {J_val_nav:0.3f}\")\n",
    "except Exception as error:\n",
    "    print(f'incorrect: please calculate J_val_nav. {error}')\n",
    "finally:    \n",
    "    print('expected Error with theta initialized with zeros: 6.871')\n",
    "    print (f\"Execution time {end_time - start_time:0.2e} nano seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorial implementation\n",
      "Error: 0.599\n",
      "expected Error with theta initialized with zeros: 6.871\n",
      "Execution time 7.17e+05 nano seconds\n"
     ]
    }
   ],
   "source": [
    "#Pseudo code for vectorized implementation to compute j_val_vec\n",
    "print (\"Vectorial implementation\")\n",
    "start_time = time.time_ns()\n",
    "\n",
    "# REPLACE THE PSEUDO CODE WITH YOUR CODE\n",
    "\n",
    "# 1.Determine the number of data points\n",
    "# 2.Determine the prediction \n",
    "# 3.Calculate the difference between this prediction and the actual value\n",
    "# 4.square this difference\n",
    "# 5.Add all these squares together and divide by twice the number of data points\n",
    "    \n",
    "### begin solution ####\n",
    "\n",
    "mse_vec = sum((X @ theta - y) ** 2) / (2*len(y))\n",
    "J_val_vec = mse_vec\n",
    "### end solution ######\n",
    "\n",
    "end_time = time.time_ns()\n",
    "try: \n",
    "    print (f\"Error: {J_val_vec:0.3f}\")\n",
    "except Exception as error:\n",
    "    print(f'incorrect: please calculate J_val_vec. {error}')\n",
    "finally:    \n",
    "    print('expected Error with theta initialized with zeros: 6.871')\n",
    "    print (f\"Execution time {end_time - start_time:0.2e} nano seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are satisfied with your algorithm that computes the cost you can implement it in a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"\n",
    "    This method calculates the cost of the current values of theta, that is, the extent to which the\n",
    "    prediction (given the specific value of theta) corresponds to the actual value (that\n",
    "    is given in y).\n",
    "\n",
    "    Every data point in X is multiplied by theta (which dimensions have X and thus theta transposed)\n",
    "    and the result of this is compared with the actual value (so with y). The difference between\n",
    "    these two values are squared and the total of all these squares is divided by it\n",
    "    number of data points to get the average. You must return this average (the variable\n",
    "    J: a number, in short).\n",
    "\n",
    "    A pseudo algorithm could be the following:\n",
    "\n",
    "    1.Determine the number of data points\n",
    "    2.Determine the prediction \n",
    "    3.Calculate the difference between this prediction and the actual value\n",
    "    4.square this difference\n",
    "    5.Add all these squares together and divide by twice the number of data points\n",
    "    \"\"\"\n",
    "    \n",
    "    mse_vec = sum((X @ theta - y) ** 2) / (2*len(y))\n",
    "    J = mse_vec\n",
    "\n",
    "\n",
    "    return J\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='03'></a>\n",
    "## Part C: implement gradient descent\n",
    "\n",
    "Next, you create the method `gradient_descent`. In this method a number of steps are performed, in each step the vector $ \\ theta $ is adjusted according to the formula below.\n",
    "\n",
    "$$ \\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j $$\n",
    "\n",
    "$ \\ Alpha $ is the learning speed (*learning rate*).\n",
    "If all goes well, every step in this method will cause the cost value $ J ( \\theta) $ to drop. Note that you edit all $ \\theta_j $ at the same time (in this case the size of $ \\theta$ is $5$, ($1$ for the intercept and $4$ for the features) so every iteration 5 parameters need to be adjusted). Also, make sure you <em> only </em> change the $ \\theta $: $X$ and $y$ are constant values that don't need to be changed.\n",
    "\n",
    "https://video.hanze.nl/media/gradient_descent/0_otuqf7wj\n",
    "\n",
    "Now finish the method `gradient_descent`, you also need to keep a list that tracks the cost of that specific iteration (the values of J) in the variable costs. We use this list to plot the cost function in the draw_costs function below to actually show the *gradual descent*. Make sure that the method `gradient_descent` returns two things: the final values of theta and the list of costs through the descent.\n",
    "\n",
    "### final note ###\n",
    "When the gradient descent function crashes because of computation errors you can use **scaled** $X$ matrix. This is a good practice anyhow. Make sure that you test the function with scaled X. Use the `reverse_theta()` to transform the terms back to the original non scaled situation. \n",
    "\n",
    "### <span style=\"background-color: lightyellow;\">Gradient descent Task</span>\n",
    "- scale X\n",
    "- complete the gradient_descent function\n",
    "- complete the test code to see the effect\n",
    "\n",
    "What will happen if you change the alpha and or the number of itterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/students/2025-2026/master/orfeas/datsc5/.venv/lib/python3.11/site-packages/pandas/core/series.py:3255: RuntimeWarning: overflow encountered in dot\n",
      "  return np.dot(lvals, rvals)\n",
      "/tmp/ipykernel_4165987/2965566220.py:24: RuntimeWarning: invalid value encountered in matmul\n",
      "  cost_history[iter] = sum((X @ theta - y) ** 2) / (2*len(y))\n",
      "/tmp/ipykernel_4165987/2965566220.py:23: RuntimeWarning: invalid value encountered in matmul\n",
      "  theta = theta - alpha / y.size * (X.T @ ((X @ theta) - y))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([nan, nan, nan, nan, nan]),\n",
       " array([1.55520501e+008, 4.27156544e+017, 1.17323898e+027, 3.22244791e+036,\n",
       "        8.85085707e+045, 2.43099882e+055, 6.67704295e+064, 1.83393353e+074,\n",
       "        5.03712829e+083, 1.38351042e+093, 3.79998475e+102, 1.04371344e+112,\n",
       "        2.86668977e+121, 7.87372274e+130, 2.16261663e+140, 5.93989763e+149,\n",
       "        1.63146733e+159, 4.48102949e+168, 1.23077091e+178, 3.38046656e+187,\n",
       "        9.28487509e+196, 2.55020731e+206, 7.00446400e+215, 1.92386383e+225,\n",
       "        5.28413312e+234, 1.45135339e+244, 3.98632398e+253, 1.09489384e+263,\n",
       "        3.00726316e+272, 8.25982518e+281, 2.26866451e+291, 6.23117141e+300,\n",
       "                    inf,             inf,             inf,             inf,\n",
       "                    inf,             inf,             inf,             inf,\n",
       "                    inf,             inf,             inf,             inf,\n",
       "                    inf,             inf,             inf,             inf,\n",
       "                    inf,             inf,             inf,             inf,\n",
       "                    inf,             inf,             inf,             inf,\n",
       "                    inf,             inf,             inf,             inf,\n",
       "                    inf,             inf,             inf,             inf,\n",
       "                    inf,             nan,             nan,             nan,\n",
       "                    nan,             nan,             nan,             nan,\n",
       "                    nan,             nan,             nan,             nan,\n",
       "                    nan,             nan,             nan,             nan,\n",
       "                    nan,             nan,             nan,             nan,\n",
       "                    nan,             nan,             nan,             nan,\n",
       "                    nan,             nan,             nan,             nan,\n",
       "                    nan,             nan,             nan,             nan,\n",
       "                    nan,             nan,             nan,             nan]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    In this problem, every parameter of theta num_iter is updated times the optimal values\n",
    "    for these parameters. Per iteration you have to update all parameters of theta.\n",
    "\n",
    "    Each parameter of theta is reduced by the sum of the error of all data points\n",
    "    multiplied by the data point itself (see the formula above).\n",
    "    This sum itself is multiplied by the 'learning rate' alpha.\n",
    "\n",
    "    A possible step-by-step plan would be:\n",
    "\n",
    "    For every iteration from 1 to num_iters:\n",
    "        1. Determine the prediction for the data point, given the current value of theta\n",
    "        2. Determine the difference between this forecast and the true value\n",
    "        3. Multiply this difference by the ith value of X.\n",
    "        4. Update the ith parameter of theta, namely by decreasing it by\n",
    "        5. alpha times the mean of the sum of the multiplication from 3\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize list of costs\n",
    "    cost_history = np.zeros(num_iters)\n",
    "    for iter in range(0, num_iters):\n",
    "            theta = theta - alpha / y.size * (X.T @ ((X @ theta) - y))\n",
    "            cost_history[iter] = sum((X @ theta - y) ** 2) / (2*len(y))\n",
    "    return theta, cost_history\n",
    "\n",
    "gradient_descent(X, y, theta, 1.0, 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Transfer scaled terms to original terms when used Standard Scaler\n",
    "When you perform linear regression on scaled data, the parameters (theta values) you obtain are in the context of the scaled features. To interpret these parameters in the context of the original (non-scaled) features, you need to transform them back.\n",
    "\n",
    "When you standardize a feature $X$, you transform it to $X' = \\frac{X - \\mu}{\\sigma}$, where $mu$ is the mean and $sigma$ is the standard deviation. The model with scaled features is $y = \\theta_0 + \\theta_1 X_1' + \\theta_2 X_2' + \\ldots$.\n",
    "\n",
    "**Intercept term**\n",
    "\n",
    "The intercept term needs special handling because it represents the bias term in the model. When scaling is applied, the mean of each feature plays a role in this term.\n",
    "The intercept adjustment formula can be derived as follows:\n",
    "\n",
    "- Substituting $X_i' = \\frac{X_i - \\mu_i}{\\sigma_i}$, you get:\n",
    "      $$y = \\theta_0 + \\theta_1 \\left(\\frac{X_1 - \\mu_1}{\\sigma_1}\\right) + \\theta_2 \\left(\\frac{X_2 - \\mu_2}{\\sigma_2}\\right) + \\ldots$$\n",
    "- Rearranging terms, the intercept $\\theta_0 $ needs to be adjusted to account for the means of the features:\n",
    "      $$\\theta_0 - \\sum \\left( \\frac{\\theta_i \\cdot \\mu_i}{\\sigma_i} \\right)$$\n",
    "\n",
    "Python: `theta_original[0] = theta_scaled[0] - np.sum((theta_scaled[1:] * scaler.mean_) / scaler.scale_)`\n",
    "\n",
    "**coefficients terms**\n",
    "\n",
    "The coefficients for the features can be transformed back by dividing by the scaling factors (standard deviations). This reverses the effect of scaling on these coefficients.\n",
    "\n",
    "Python: `theta_original[1:] = theta_scaled[1:] / scaler.scale_`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_theta(theta, scaler):\n",
    "    \"\"\"\n",
    "    Transforms the theta parameters obtained from a regression model on scaled data \n",
    "    back to the original (non-scaled) feature space.\n",
    "\n",
    "    Parameters:\n",
    "    theta (numpy array): Array of theta parameters obtained from the regression model.\n",
    "    scaler (StandardScaler object): The scaler object used to scale the features.\n",
    "\n",
    "    Returns:\n",
    "    numpy array: transformed theta parameters in the context of the original, non-scaled features.\n",
    "    \n",
    "    The function performs the following steps:\n",
    "    1. Initializes a new array `theta_original` with the same shape as `theta`.\n",
    "    2. Corrects the intercept (theta[0]) to account for the means of the original features.\n",
    "    3. Adjusts the coefficients (theta[1:]) by reversing the effect of scaling using the standard deviations.\n",
    "    \"\"\"\n",
    "    #initialize\n",
    "    theta_original = np.zeros_like(theta)\n",
    "    #tranform back intercept\n",
    "    theta_original[0] = theta[0] - np.sum((theta[1:] * scaler.mean_) / scaler.scale_)\n",
    "    #transform back coefficients\n",
    "    theta_original[1:] = theta[1:] / scaler.scale_\n",
    "    \n",
    "    return theta_original\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your code\n",
    "\n",
    "Congratulations! You've written the core of the linear regression function. Now, we can test the code using the Delaney dataset. Remember to scale the X matrix before running the tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.07841938 -0.358581   -0.82777768 -1.06346051]\n",
      " [ 1.         -0.0389415  -0.69438198 -0.82777768 -1.06346051]\n",
      " [ 1.          0.07756156 -0.358581   -0.4470066  -1.06346051]\n",
      " ...\n",
      " [ 1.          0.62217181  1.01097421  0.69530662  0.96376327]\n",
      " [ 1.          0.06058739  1.46412275  0.31453555  0.45695732]\n",
      " [ 1.         -0.22919587 -0.24774572 -0.4470066   0.2815245 ]]\n",
      "Theta: [-3.05799738 -1.3828273  -0.67682451  0.0081764  -0.14538803]\n",
      "Theta original: [ 0.25582727 -0.74138799 -0.0065983   0.00311334 -0.42368024]\n",
      "Final cost: 0.5053239759010153\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected outcome with zero initialized theta, alpha 0.01 and 10000 itterations:\n",
      "Final cost: 0.5\n",
      "Theta original: [ 0.25846249 -0.74029875 -0.00660125  0.00226283 -0.43112639]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE TO CREATE scaled_X, y and theta \n",
    "scaler = StandardScaler()\n",
    "# define X, y, theta, and scale X\n",
    "### begin solution ###\n",
    "X, y, theta = regr.prep_data(df_logS, y_col=\"logS\")\n",
    "m, n = X.shape\n",
    "theta = np.zeros(n)\n",
    "\n",
    "X_scaled, theta, scaler = regr.scale_x(X)\n",
    "print(X_scaled)\n",
    "\n",
    "### end solution ####\n",
    "\n",
    "# Set hyperparameters\n",
    "alpha = 0.01  # Learning rate\n",
    "num_iters = 10000  # Number of iterations\n",
    "\n",
    "\n",
    "try: \n",
    "    # Run gradient descent\n",
    "    theta, J_history = regr.fit(X_scaled, y, theta, alpha=alpha, num_iters=num_iters)\n",
    "    # Final parameters\n",
    "    print(\"Theta:\", theta)\n",
    "    print(f\"Theta original: {regr.reverse_theta(theta, scaler)}\")\n",
    "\n",
    "    # Final cost\n",
    "    print(\"Final cost:\", J_history[-1])\n",
    "    draw_costs(J_history)\n",
    "# except Exception as error:\n",
    "#     print(f'something went wrong, try again. {error}')\n",
    "finally: \n",
    "    print('Expected outcome with zero initialized theta, alpha 0.01 and 10000 itterations:')\n",
    "    print('Final cost: 0.5')\n",
    "    print('Theta original: [ 0.25846249 -0.74029875 -0.00660125  0.00226283 -0.43112639]')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### critical note\n",
    "\n",
    "The delaney formula differs. This should be addressed somewhere in the evaluation document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a name='04'></a>\n",
    "## Part D: implement regularization\n",
    "\n",
    "Regularization works by adding the penalty term to the cost function used in linear regression. The penalty term controls the magnitude of the coefficients. The regularization term prevents the coefficients from becoming too large. \n",
    "\n",
    "There are three regularization methods. Lasso, Ridge and Elastic net\n",
    "The ridge regression formula is \n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} ( h_\\theta(x^{(i)}) - y^{(i)} ) ^2 + \\lambda \\sum_{j=1}^{m}\\theta_j^2$$\n",
    "\n",
    "where the regularization term is \n",
    "$$ \\lambda \\sum_{j=1}^{m}\\theta_j^2$$ \n",
    "\n",
    "In gradient descent, you update the parameters $theta_j$ (except for the intercept term) by subtracting the learning rate times the gradient. The update rules are as follows:\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha \\left( \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} + \\frac{\\lambda}{m} \\theta_j \\right) \\quad \\text{for} \\; j \\geq 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_0 := \\theta_0 - \\alpha \\left( \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "A higher value of λ increases the penalty on the coefficients, effectively shrinking them towards zero but never exactly zero, thus preventing overfitting by reducing the model complexity. A lower value of λ reduces the impact of the regularization term, allowing the model to fit the data more closely, which could lead to overfitting if the model becomes too complex. λ = 1 is considered as moderate regularization.\n",
    "\n",
    "\n",
    "\n",
    "https://video.hanze.nl/media/regularization.m4v/0_cv10hwxn\n",
    "\n",
    "\n",
    "### <span style=\"background-color: lightyellow;\">Regularization Task</span>\n",
    "- update the `compute_cost` function and `gradient_descent` function with regularization (best is to copy the function to keep the original)\n",
    "- test the functions to see the effect\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/students/2025-2026/master/orfeas/datsc5/.venv/lib/python3.11/site-packages/numpy/_core/_methods.py:51: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/tmp/ipykernel_4165987/3573281435.py:17: RuntimeWarning: overflow encountered in square\n",
      "  cost = (np.sum((X @ theta - y) ** 2) + lambda_ * np.sum(theta[1:] ** 2)) / (2 * m)\n",
      "/students/2025-2026/master/orfeas/datsc5/.venv/lib/python3.11/site-packages/pandas/core/series.py:3255: RuntimeWarning: overflow encountered in dot\n",
      "  return np.dot(lvals, rvals)\n",
      "/students/2025-2026/master/orfeas/datsc5/.venv/lib/python3.11/site-packages/pandas/core/series.py:3255: RuntimeWarning: invalid value encountered in dot\n",
      "  return np.dot(lvals, rvals)\n",
      "/tmp/ipykernel_4165987/3573281435.py:15: RuntimeWarning: invalid value encountered in subtract\n",
      "  theta = theta - alpha * (grad + reg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([nan, nan, nan, nan, nan]),\n",
       " array([3.42618003e+013, 9.41043302e+022, 2.58469341e+032, 7.09918452e+041,\n",
       "        1.94988004e+051, 5.35558999e+060, 1.47097994e+070, 4.04023083e+079,\n",
       "        1.10970005e+089, 3.04793032e+098, 8.37152278e+107, 2.29934369e+117,\n",
       "        6.31543573e+126, 1.73461360e+136, 4.76433375e+145, 1.30858400e+155,\n",
       "        3.59419001e+164, 9.87189351e+173, 2.71143932e+183, 7.44730805e+192,\n",
       "        2.04549653e+202, 5.61821267e+211, 1.54311255e+221, 4.23835209e+230,\n",
       "        1.16411654e+240, 3.19739203e+249, 8.78203810e+258, 2.41209687e+268,\n",
       "        6.62512648e+277, 1.81967405e+287, 4.99796293e+296,             inf,\n",
       "                    inf,             inf,             inf,             inf,\n",
       "                    inf,             inf,             inf,             inf,\n",
       "                    inf,             inf,             inf,             inf,\n",
       "                    inf,             inf,             inf,             inf,\n",
       "                    inf,             inf,             inf,             inf,\n",
       "                    inf,             inf,             inf,             inf,\n",
       "                    inf,             inf,             inf,             inf,\n",
       "                    inf,             inf,             inf,             inf,\n",
       "                    inf,             nan,             nan,             nan,\n",
       "                    nan,             nan,             nan,             nan,\n",
       "                    nan,             nan,             nan,             nan,\n",
       "                    nan,             nan,             nan,             nan,\n",
       "                    nan,             nan,             nan,             nan,\n",
       "                    nan,             nan,             nan,             nan,\n",
       "                    nan,             nan,             nan,             nan,\n",
       "                    nan,             nan,             nan,             nan,\n",
       "                    nan,             nan,             nan,             nan]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_cost(X, y, theta, lambda_):\n",
    "    # YOUR CODE HERE\n",
    "    ### begin solution ####\n",
    "    return X\n",
    "    ### end solution\n",
    "\n",
    "def gradient_descent_reg(X, y, theta, alpha, num_iters, lambda_):\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(num_iters)\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        grad = (X.T @ ((X @ theta) - y)) / m\n",
    "        reg = (lambda_ / m) * theta\n",
    "        reg[0] = 0  \n",
    "        theta = theta - alpha * (grad + reg)\n",
    "        \n",
    "        cost = (np.sum((X @ theta - y) ** 2) + lambda_ * np.sum(theta[1:] ** 2)) / (2 * m)\n",
    "        cost_history[i] = cost\n",
    "    \n",
    "    return theta, cost_history\n",
    "\n",
    "gradient_descent_reg(X, y, theta, 1.0, 100, 0.5)\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# prepare the data \n",
    "# test the function \n",
    "# print and plot the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Regression\n",
    "By now you created code to compute the cost, the gradient_descent to update theta and you included a regularization factor. Best practise is to compose a regression class with all methods and parameters to set (learningrate, lambda, number of itterations), including some feedback methods (transfered back coefficients and cost function plot). Use a method `fit()` to create a linear regresssion model equation and `predict` to use the model. Evaluate the usage and performance of your model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CLASS HERE \n",
    "# YOUR TEST CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Logistic Regression\n",
    "---\n",
    "<a name='051'></a>\n",
    "## Part E: Use case logistic regression\n",
    "\n",
    "The logistic regression use case in this notebook is based on data from the `ChemBL` database [2], a comprehensive resource that provides information on the bioactivity of small molecules and their biological targets. Specifically, we will focus on `CHEMBL2034`[2], which pertains to the Glucocorticoid Receptor (GR), a nuclear receptor that regulates gene expression in response to steroid hormones. The GR plays a critical role in various physiological processes, including inflammation, metabolism, and immune response.\n",
    "\n",
    "Understanding the bioactivity of compounds targeting the GR is essential for drug discovery and the development of therapeutics aimed at modulating its activity. The CHEMBL2034 dataset includes molecular structure data (SMILES) and bioactivity data. By analyzing structure-activity relationships (SAR), researchers can understand how modifications in a molecule's structure influence its bioactivity. This knowledge is crucial for optimizing drug candidates. In this use case we will predict if the molecule developes significant bioactivity or not.  \n",
    "\n",
    "To use the ChemBL database we can use the chembl_webresource_client API. The API fetches bioactivity data for compounds targeting the GR, filtered by IC50 values. Fields from the dataset we will work with are the `canonical_smiles` providing the molecular structure, and `standard_value` (IC50) which is used to determine bioactivity. We transfer bioactivity to 0 or 1, in order to use it as a class. The `rdkit` (rdkit.org/docs) [4] can be used to compute features from the SMILE structure. \n",
    "\n",
    "SMILES (Simplified Molecular Input Line Entry System): A notation that provides a way to describe a molecule's structure using a string of text\n",
    "\n",
    "[2] Gaulton, A., et all, 2012. ChEMBL: a large-scale bioactivity database for drug discovery. Nucleic acids research, 40(D1), pp.D1100-D1107.<br></br>\n",
    "[3] Landrum, G., 2013. Rdkit documentation. Release, 1(1-79), p.4.\n",
    "\n",
    "\n",
    "### <span style=\"background-color: lightyellow;\">Data preperation Task</span>\n",
    "\n",
    "- **Review the Script:** Carefully examine the script provided below to understand how the data is featurized. There is no need to execute the code; focus on grasping the transformation and feature extraction processes involved.\n",
    "\n",
    "- **Load the Data:** Download the `CHEMBL2034.csv` file from Blackboard and load it into this notebook.\n",
    "- Conduct a thorough **inspection** of the dataset to familiarize yourself with its structure and contents. \n",
    "- **Data Cleaning:** Based on your inspections, perform the necessary data cleaning steps to ensure the dataset is ready for analysis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chembl_webresource_client'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### CODE THAT WAS USED TO PREPARE THE DATA FILE \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# imports to retrieve data from chemBL\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchembl_webresource_client\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnew_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m new_client\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrdkit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chem\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrdkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mChem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Descriptors\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chembl_webresource_client'"
     ]
    }
   ],
   "source": [
    "### CODE THAT WAS USED TO PREPARE THE DATA FILE \n",
    "# imports to retrieve data from chemBL\n",
    "from chembl_webresource_client.new_client import new_client\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "\n",
    "# Initialize the ChEMBL client\n",
    "chembl = new_client\n",
    "\n",
    "target_chembl_id = 'CHEMBL2034' #Glucocorticoid receptor targets\n",
    "\n",
    "# Get bioactivity data for the target\n",
    "activity = chembl.activity.filter(target_chembl_id=target_chembl_id, standard_type='IC50')\n",
    "\n",
    "# Convert to DataFrame\n",
    "import pandas as pd\n",
    "df_activity = pd.DataFrame(activity)\n",
    "display(df_activity.head(2))\n",
    "\n",
    "# Select relevant columns and drop rows with missing values\n",
    "df_activity = df_activity[['molecule_chembl_id', 'canonical_smiles', 'standard_value']]\n",
    "df_activity.dropna(inplace=True)\n",
    "\n",
    "# Convert IC50 values to binary labels (active if IC50 < 1000 nM, inactive otherwise)\n",
    "df_activity['bioactivity_label'] = df_activity['standard_value'].apply(lambda x: 1 if float(x) < 1000 else 0)\n",
    "\n",
    "display(df_activity.head(2))\n",
    "\n",
    "# Function to calculate molecular descriptors for smiles\n",
    "def calculate_descriptors(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    descriptors = {\n",
    "        'MolecularWeight': Descriptors.MolWt(mol),\n",
    "        'NumHDonors': Descriptors.NumHDonors(mol),\n",
    "        'NumHAcceptors': Descriptors.NumHAcceptors(mol),\n",
    "        'TPSA': Descriptors.TPSA(mol)\n",
    "    }\n",
    "    return pd.Series(descriptors)\n",
    "\n",
    "# Calculate descriptors for each molecule\n",
    "df_descriptors = df_activity['canonical_smiles'].apply(calculate_descriptors)\n",
    "df_GR= pd.concat([df_descriptors, df_activity], axis=1)\n",
    "df_GR.drop(columns=['standard_value', 'molecule_chembl_id'], inplace=True)\n",
    "\n",
    "print('final dataframe')\n",
    "display(df_GR.head(2))\n",
    "# df_GR.to_csv('CHEMBL2034.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a name='05'></a>\n",
    "## Part F: implement logistic regression\n",
    "\n",
    "Logistic regression uses the logistic function (sigmoid function) to map predicted values to probabilities. The logistic function is defined as:\n",
    "\n",
    "$$ g (z) = \\frac {1} {1 + e ^ {- z}} $$\n",
    "\n",
    "where $z$ is the linear combination of the input features, similar to linear regression\n",
    "$$z = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_n x_n$$\n",
    "\n",
    "\n",
    "\n",
    "The terms (intercept and coefficients) are also calculated with a `compute_cost` function and `gradient_descent` function, but the coefficients are calculated differently. In logistic regression, coefficients represent the log odds. Positive coefficients increase the log odds of the outcome being 1, while negative coefficients decrease the log odds. \n",
    "\n",
    "The cost function is computed as:\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2 \n",
    "$$\n",
    "\n",
    "in python\n",
    "```{python}\n",
    "h = sigmoid(X.dot(theta))\n",
    "cost = -1 / m * (y.dot(np.log(h)) + (1 - y).dot(np.log(1 - h)))\n",
    "```\n",
    "\n",
    "\n",
    "and the gradient descent is computed for the intercept term ($\\theta_0$):\n",
    "\n",
    "  $$\n",
    "  \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})\n",
    "  $$\n",
    "\n",
    "and for the other parameters ($\\theta_j$ where $j \\geq 1$) (including regularization):\n",
    "\n",
    "  $$\n",
    "  \\theta_j := \\theta_j - \\alpha \\cdot \\left( \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} + \\frac{\\lambda}{m} \\theta_j \\right)\n",
    "  $$\n",
    "\n",
    "where $h$ again is the result of the sigmoid function. The gradients for logistic regression and regression are similar, but the prediction $h_\\theta(x^{(i)})$ is the output of the sigmoid function rather than a linear output. Basically it is a probability rather than a direct numeric prediction.\n",
    "\n",
    "The output of logistic regression can be interpreted as the probability of the target variable being 1. This probability can be converted into a binary decision by setting a threshold \n",
    "Commonly, a threshold of 0.5 is used, where:\n",
    "- If $P(y=1|x) \\geq 0.5$, classify as 1.\n",
    "- If $P(y=1|x) < 0.5$, classify as 0.\n",
    "\n",
    "### <span style=\"background-color: lightyellow;\">Logistic regression Task</span>\n",
    "\n",
    "- Implement the `sigmoid` function in the cell below. Make it so that you can add both a number and a vector to it. In the first case, the function must return the sigmoid value of the number, in the second case it must return a vector containing the sigmoid value of each individual element in the input vector. You can use the numpy function `exp()`.\n",
    "- Complete `compute_cost` and `gradient_descent` function for logistic regression\n",
    "- Complete the `predict` function. \n",
    "- Test your code \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    # YOUR CODE HERE\n",
    "    ### begin solution\n",
    "    sig = 1/(1+np.exp(-z))\n",
    "    ### end solution \n",
    "    return sig\n",
    "\n",
    "#code to validate function\n",
    "try:    \n",
    "    nums = np.arange(-10, 10, step=.5)\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    ax.plot(nums, sigmoid(nums), 'r')\n",
    "except:\n",
    "    print('make sure that the sigmoid function returns the sigmod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the sigmoid function in the other functions. Complete the functions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x, y, theta, lambda_):\n",
    "    \"\"\"\n",
    "    Compute cost for logistic regression with regularization.\n",
    "\n",
    "    Parameters:\n",
    "    X:  Input feature matrix (m x n)\n",
    "    y: True labels vector (m,)\n",
    "    theta: Parameters vector (n,)\n",
    "    lambda_: Regularization parameter\n",
    "\n",
    "    Return:\n",
    "    J:Cost value\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    h = sigmoid(x.dot(theta))\n",
    "    reg = (lambda_ / (2 * m)) * np.sum(theta[1:] ** 2)\n",
    "    cost = -1 / m * (y.dot(np.log(h)) + (1 - y).dot(np.log(1 - h)))\n",
    "    return cost + reg\n",
    "\n",
    "\n",
    "\n",
    "def gradient_descent(x, y, theta, alpha, num_iters, lambda_):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to find optimal theta.\n",
    "\n",
    "    Parameters:\n",
    "    X: Input feature matrix (m x n)\n",
    "    y: True labels vector (m,)\n",
    "    theta: Initial parameters vector (n,)\n",
    "    alpha: Learning rate\n",
    "    num_iters: Number of iterations\n",
    "    lambda_:  Regularization parameter\n",
    "\n",
    "    Return:\n",
    "    theta:  Updated parameters vector\n",
    "    J_history:  History of cost values\n",
    "    \"\"\"\n",
    "\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(num_iters)\n",
    "    for i in range(num_iters):\n",
    "        grad = (x.T @ ((sigmoid(x @ theta)) - y)) / m\n",
    "        reg = (lambda_ / m) * theta\n",
    "        reg[0] = 0\n",
    "        theta = theta - alpha * (grad + reg)\n",
    "        cost_history[i] = compute_cost(x, y, theta, lambda_)\n",
    "    return theta, cost_history\n",
    "\n",
    "def predict(x, theta, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters theta.\n",
    "\n",
    "    Parameters:\n",
    "    X: Input feature matrix (m x n)\n",
    "    theta: Parameters vector (n,) (computed by compute cost and gradient descentl)\n",
    "    threshold: Threshold for prediction\n",
    "\n",
    "    Return:\n",
    "    p: Predicted labels vector (m,)\n",
    "    \"\"\"\n",
    "    pred = sigmoid(x.dot(theta))\n",
    "    p = [1.0 if h >= threshold else 0.0 for h  in pred ]\n",
    "    return p\n",
    "\n",
    "#BONUS\n",
    "def predict_proba():\n",
    "    #adjust and complete this function\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_GR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m num_iters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15000\u001b[39m  \u001b[38;5;66;03m# Number of iterations\u001b[39;00m\n\u001b[1;32m      5\u001b[0m lambda_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m  \u001b[38;5;66;03m# Regularization parameter\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m df_GR_new \u001b[38;5;241m=\u001b[39m \u001b[43mdf_GR\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcanonical_smiles\u001b[39m\u001b[38;5;124m\"\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m regr \u001b[38;5;241m=\u001b[39m Regr()\n\u001b[1;32m      9\u001b[0m x, y, theta \u001b[38;5;241m=\u001b[39m regr\u001b[38;5;241m.\u001b[39mprep_data(df_GR_new, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbioactivity_label\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_GR' is not defined"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE TO PREPARE X_scaled, y, theta\n",
    "# Set hyperparameters, adjust to imporve\n",
    "alpha = 0.01  # Learning rate\n",
    "num_iters = 15000  # Number of iterations\n",
    "lambda_ = 0.7  # Regularization parameter\n",
    "\n",
    "df_GR_new = df_GR.drop(\"canonical_smiles\", axis=1)\n",
    "regr = Regr()\n",
    "x, y, theta = regr.prep_data(df_GR_new, \"bioactivity_label\")\n",
    "X_scaled, theta, scaler = regr.scale_x(x)\n",
    "\n",
    "try: \n",
    "    # Train the model\n",
    "    theta, J_history = regr.fit(X_scaled, y, theta, alpha, num_iters=num_iters, lambda_ = lambda_)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = regr.predict(X_scaled, theta)\n",
    "\n",
    "    # Evaluate the model\n",
    "    acc = accuracy(y, predictions)\n",
    "    print(f'Accuracy is {acc:.2f}')\n",
    "\n",
    "\n",
    "    draw_costs(J_history)\n",
    "    print(f'error:{J_history[-1]}')\n",
    "# except Exception as error: \n",
    "#     print(f'something is wrong. {error}')\n",
    "finally:\n",
    "    print('Depending on hyperparameters accuracy should be around 0.75 +/- 1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='06'></a>\n",
    "## Part G: compare with sklearn\n",
    "\n",
    "Scikit-Learn, often referred to as sklearn, is a powerful and widely-used open-source machine learning library for Python. It contains regression models as well. \n",
    "See https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html and \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "\n",
    "### <span style=\"background-color: lightyellow;\">Evaluation Task</span>\n",
    "Compare the performance (error) and coefficients (theta values) of your regression model to the error and coefficients of the sklearn regression model. \n",
    "\n",
    "Next compare the performance  of your logistic regression model to your own developed model. As a minimum compare accuracy.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "\n",
    "### Notes: \n",
    "Best practise is to split your data into a train set and test set. <br></br>\n",
    "The models differ from the original delaney paper. Can you think of any reason? \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE TO EVALUATE REGRESSION MODEL\n",
    "# expected outcome is that both models should procude similar outcomes\n",
    "\n",
    "### begin solution\n",
    "\n",
    "\n",
    "### end solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE TO EVALUATE LOGISTIC REGRESSION MODEL\n",
    "\n",
    "### begin solution\n",
    "\n",
    "\n",
    "### end solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='07'></a>\n",
    "## Part H: Development of the logistic regression class\n",
    "\n",
    "When you are satisfied with your developed code snippets you can build a python file (No notebook!!!) with the logistic regression class. Use your Object Oriented programming skills to compose a class with at least the methods `fit()` and `predict()`.  The method `fit()` should fit the best logistic regression equation and the method `predict()` should predict the class. Add the `predict_proba()` method optionally . \n",
    "\n",
    "### <span style=\"background-color: lightyellow;\">Development Task</span>\n",
    "Submit a repository with a directory `regression` that includes:\n",
    "- The logistic regression class (python file) with proper documentation.\n",
    "- This completed notebook for reference\n",
    "- An evaluation document that details specific scenarios or datasets where each type of regression (linear, logistic, polynomial) is appropriate or inappropriate.\n",
    "\n",
    "### Additional notes\n",
    "- Code quality and readability are important, so adhere to best practices in coding and documentation.\n",
    "- If you want to optimize your model you could consider the deepchem featurizer (not mandatory, just for fun)\n",
    "- use an argumentative approach in your notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='08'></a>\n",
    "## Part I: Development of the polynomial regression class\n",
    "\n",
    "This part is the bonus part, and it is not mandatory. \n",
    "\n",
    "Make sure that you understand the theoretical background of the polynomial regression. \n",
    "In addition to the logistic regression develop your own polynomial regression class. Demonstrate the performance and usage with a use case. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## References:\n",
    "- Numpy: https://nbviewer.jupyter.org/github/ageron/handson-ml/blob/master/tools_numpy.ipynb\n",
    "- Linear regression: https://video.hanze.nl/media/linear+regression/0_63duegik\n",
    "- vectorized implementation: https://video.hanze.nl/media/why_we_love_numpy.mov/0_npz2lx0a\n",
    "- gradient descent: https://video.hanze.nl/media/gradient_descent/0_otuqf7wj\n",
    "- regularization https://video.hanze.nl/media/regularization.m4v/0_cv10hwxn\n",
    "\n",
    "[1]. Delaney, John S. “ESOL: estimating aqueous solubility directly from molecular structure.” Journal of chemical information and computer sciences 44.3 (2004): 1000-1005.<br></br>\n",
    "[2] Gaulton, A., et all, 2012. ChEMBL: a large-scale bioactivity database for drug discovery. Nucleic acids research, 40(D1), pp.D1100-D1107.<br></br>\n",
    "[3] Landrum, G., 2013. Rdkit documentation. Release, 1(1-79), p.4.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
